This guide outlines the best practices for implementing and managing active-active replication using pgactive.

## Best Practices
- pgactive is not a drop-in solution for all applications. Applications that use pgactive for an active-active database cluster must make specific design decisions to ensure they can operate safely. Even if you are routing write traffic to a single pgactive instance, you must make sure your application is architected to support an active-active replication topology. This section provides an overview of pgactive functionality and what design considerations you must make for your applications that use pgactive.
- When using pgactive, you must plan for conflicts and how to resolve them. Each database instance that uses pgactive is an independent instance and can accept data changes from any source. When a change is sent to a database instance, PostgreSQL commits it locally and then uses pgactive to replicate the change asynchronously to another system. Although using asynchronous replication between two PostgreSQL instances with pgactive is helpful for performance and availability, it’s possible for two PostgreSQL instances to update the same record at the same time.
- pgactive provides mechanisms for conflict detection and automatic resolution. pgactive tracks the timestamp of when the transaction was committed on both systems, and will automatically apply the change of the latest timestamp. pgactive also logs when a conflict occurs in the pgactive.pgactive_conflict_history table, so you can manually choose a different solution. Even though pgactive has automatic conflict resolution, part of your application may have received stale data, so you should design your application to account for this case.
- As part of conflict detection, pgactive must inspect any rows that have changes. This requires loading any affected row to perform the conflict detection. pgactive does this through the use of a primary key, which is a constraint that acts as a unique identifier for every row in a table, because using a primary key is a performant way to retrieve a row. pgactive requires that every table use a primary key, so your application must ensure all tables include a primary key constraint. Additionally, to prevent a certain kind of conflict that could produce duplicate or missing rows, pgactive prevents updates directly to a column that has a primary key constraint. Although most application development frameworks will automatically add primary keys to tables, you should ensure that applications you intend to use with pgactive include this constraint.
- Developers often use integer sequences as their primary keys in PostgreSQL, for example, the PostgreSQL serial and bigserial types or via the GENERATED BY DEFAULT AS IDENTITY SQL-standard syntax. However, integer sequences that increment by 1, which is the default in PostgreSQL, will introduce conflicts in an active-active replication deployment. pgactive has functions to create nonconflicting sequences and convert sequences to nonconflicting sequences (SELECT pgactive.convert_local_seqs_to_snowflake_id_seqs()). Additionally, pgactive requires that integer-based sequences are 64-bit (bigint) to mitigate the risk of sequence wraparound. You can also choose to use other identifiers, such as UUIDs.
- pgactive doesn’t allow you to replicate schema changes (data definition language or DDL commands). Applying changes to schemas requires either a coordinated rollout using a two-phase commit (2PC).
- pgactive doesn’t provide any support for large object commands, so your application should use another storage mechanism such as a bytea type or Amazon Simple Storage Service (Amazon S3) for storing large objects. pgactive ensures that all instances use the same version of any text collation providers, and will prevent an instance from joining if the provider version doesn’t match.
- Finally, you’ll need to make decisions about your network topology that can help optimize performance and correctness. Network latency between PostgreSQL instances using pgactive for active-active replication can directly impact correctness, because higher latency increases the risk of introducing conflicts. Additionally, large transactions can impact replication lag, because each affected row must be replayed and applied on a target instance. You should not use pgactive for load balancing writes between multiple instances, because this doesn’t improve write performance and greatly increases the risk of introducing conflicts.

### 1. When to Use pgactive

Before implementation, ensure your use case aligns with pgactive’s capabilities.

* Ideal Use Cases:
* Best for OLTP load. OLAP load can be handled with proper design of bulk data loading.
* Global Write Availability: Accepting writes in multiple regions to reduce latency.
* High Availability: Instant failover where applications simply redirect writes to a surviving node.
* Blue/Green Deployments: Migrating data between systems that must both remain writable during the transition.

* Not a Magic Bullet: It is asynchronous. It does not provide strong consistency across nodes (CAP theorem applies). Application logic *must* tolerate eventual consistency and potential conflicts.

### 2. System Resources

- Allocate sufficient CPU, RAM, Network, and Storage

- Each node in pgactive cluster has extra background processes running.
    - A supervisor - To manage pgactive
    - WAL Sender - one process per participating node
    - Apply Worker - one process per participating node
Ex: 
    - on a two node system, each node will have three extra processes
    - on a three node system, each node will have five extra processes

- Adjust `wal_sender_timeout` and `wal_receiver_timeout` based on your network latency.

- Adjust `logical_decoding_work_mem` (for PostgreSQL >= 14 monitor `pg_stat_replication_slots` spill_* stats).

### 3. Schema Design & Application Logic

Active-active replication fundamentally changes how you must design your database schema.

* Primary Keys are Mandatory:
* Requirement: Every table *must* have a `PRIMARY KEY`.
* Why: Logical replication requires a unique identifier to apply updates and deletes reliably. Without it, replication will fail or become dangerously slow (full table scans).


* Handling Sequences (Crucial):
* The Problem: Standard `SERIAL` or `IDENTITY` columns will generate duplicate IDs on different nodes, causing Unique Constraint violations.
* 1 (Recommended): Use UUIDs (v4 or v7) as primary keys. They are globally unique by design and avoid collision entirely.
* 2: Use pgactive’s built-in snowflake ID generators to ensure unique keys.
* 3: If you must use integers, configure Step and Offset manually to ensure Node A generates `1, 11, 21...` and Node B generates `2, 12, 22...`.


* DDL Management:
* Manual Propagation: Schema changes (CREATE/ALTER TABLE) are not automatically replicated to other nodes.
* Workflow: You must run DDL statements on all nodes independently.
* Order of Operations:
1. Apply non-breaking changes (e.g., adding a nullable column) to all nodes.
2. Wait for replication to catch up.
3. Deploy application code that uses the new schema.

### 4. Configuration & Setup

* Database Parameters:
* `wal_level = logical` (Required for logical replication).
* `track_commit_timestamp = on` (Critical for conflict resolution).
* `shared_preload_libraries` must include `pgactive`.
* `max_replication_slots` and `max_wal_senders`: Set these high enough to accommodate all your active nodes plus spares for maintenance.

* Conflict Resolution Strategy:
* Default: `last-update-wins`. The transaction with the newer commit timestamp overwrites the older one.
* Implication: You must ensure NTP (Time) Synchronization across all servers. If clocks drift, "last write" becomes arbitrary.

### 5. Operational Best Practices

* Avoid long running transactions:
* Only completed transactions are replicated. Long running transactions may have longer replication delays because replication doesn't start until the transaction completes.
* Long running queries may also result in replication delays.

* Conflict Avoidance (Geo-Sharding):
* Just because you *can* write to any node doesn't mean you *should* write to the same row from multiple nodes simultaneously.
* Strategy: Route users to a specific "home" region based on their ID. Only write their data to that region. Use the other regions for read-only access or failover. This minimizes conflicts significantly.

* Monitoring:
* Replication Lag: Monitor the lag closely. High lag increases the "conflict window" (the time during which a concurrent write on another node causes a conflict).
* Conflict History: Monitor the `pgactive_conflict_history` table. Frequent entries here indicate a flaw in your application's write routing or schema design.
* Replication Slots: Watch for `active = false` or growing WAL usage. If a node goes down, the replication slot on the other nodes will retain WAL logs until the disk fills up.

* Backup & Recovery:
* Active-active is for *availability*, not *backup*. If you accidentally `DELETE FROM users` on Node A, that deletion replicates to Node B instantly.
* Maintain distinct Point-in-Time Recovery (PITR) backups for at least one node.

### 6. Limitations & Pitfalls

* No "Cascading" Logical Replication: Avoid complex topologies where Node A replicates to Node B, which replicates to Node C. Stick to a mesh topology where all nodes connect directly if possible.
* Large Objects (LOBs): PostgreSQL logical replication does not support Large Objects. Store large files in S3/Blob storage and keep only the reference in the DB.
* Truncate: `TRUNCATE` commands must be used with extreme caution or avoided, as they can behave unexpectedly depending on the replication set configuration.

### Quick Checklist for Production Readiness

1. [ ] All tables have Primary Keys.
2. [ ] Sequences are replaced with UUIDs or configured with offsets.
3. [ ] `track_commit_timestamp` is ON.
4. [ ] NTP is synced on all nodes.
5. [ ] Application logic handles "eventual consistency" (stale reads are possible).
6. [ ] Monitoring is set up for `pgactive_get_replication_lag_info()` and `pgactive_conflict_history`.
